{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216eb04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"spacytextblob\")\n",
    "tqdm.pandas()\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "        repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\"\n",
    "    )\n",
    "\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "def get_probs(text, model):\n",
    "    text = \" \".join(text.split(\"\\n\"))\n",
    "    langs, probs = model.predict(text, k=157)\n",
    "    np.asarray(probs)\n",
    "    \n",
    "    if \"__label__eng_Latn\" in langs:\n",
    "        return probs[langs.index(\"__label__eng_Latn\")]\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b7db40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AU | Google | Sentiment | 946 | 130 | 270 | 0.7347696879643388 | 0.9983530515281797\n",
      "AU | Google | Sarcasm | 946 | 130 | 270 | 0.07280832095096583 | 0.9983530515281797\n",
      "AU | Reddit | Sentiment | 1763 | 241 | 501 | 0.3193612774451098 | 0.9787314457212765\n",
      "AU | Reddit | Sarcasm | 1763 | 241 | 501 | 0.42035928143712575 | 0.9787314457212765\n",
      "IN | Google | Sentiment | 1648 | 225 | 469 | 0.7480785653287788 | 0.992057483665715\n",
      "IN | Google | Sarcasm | 1647 | 225 | 469 | 0.007689021785561726 | 0.9920541094645431\n",
      "IN | Reddit | Sentiment | 1685 | 230 | 479 | 0.2543859649122807 | 0.8755089748438211\n",
      "IN | Reddit | Sarcasm | 1686 | 230 | 479 | 0.13319415448851774 | 0.8755605909933547\n",
      "UK | Google | Sentiment | 1817 | 248 | 517 | 0.7482571649883811 | 0.9990422319617962\n",
      "UK | Google | Sarcasm | 1821 | 249 | 518 | 0.00115919629057187 | 0.9990438435002125\n",
      "UK | Reddit | Sentiment | 1007 | 138 | 287 | 0.11452513966480447 | 0.9749663745411852\n",
      "UK | Reddit | Sarcasm | 1031 | 141 | 294 | 0.2203274215552524 | 0.9755218465871965\n"
     ]
    }
   ],
   "source": [
    "for loc in [\"au\", \"in\", \"uk\"]:\n",
    "    for dom in [\"google\", \"reddit\"]:\n",
    "        for task in [\"sentiment\", \"sarcasm\"]:\n",
    "            size = {}\n",
    "            df = []\n",
    "            for split in [\"train\", \"valid\", \"test\"]:\n",
    "                data = pd.read_csv(f\"./splits/{task.title()}/en-{loc.upper()}/{dom.title()}/{split}.csv\", encoding=\"ascii\", encoding_errors=\"ignore\")\n",
    "                size[split] = len(data)\n",
    "                df.append(data)\n",
    "\n",
    "            df = pd.concat(df).reset_index(drop = True)\n",
    "            \n",
    "            lang = df[\"text\"].apply(lambda x : get_probs(x, model)).mean()\n",
    "\n",
    "            print(f\"{loc.upper()} | {dom.title()} | {task.title()} | {size['train']} | {size['valid']} | {size['test']} |\", df[\"label\"].value_counts().get(1, 0)/len(df), f\"| {lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3119327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
