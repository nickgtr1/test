{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddec19e-7099-4260-99ed-6155468e71aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, AutoTokenizer, EarlyStoppingCallback, DataCollatorWithPadding, Trainer, TrainingArguments\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, EarlyStoppingCallback, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ab70f-d9b9-41e4-9a8b-b7cd34019e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c2a27-796b-424b-a20f-f6c8015e48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "def predict(texts, model, tokenizer):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts):\n",
    "            inputs = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                return_token_type_ids=False,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = {\n",
    "                key: value.to(device) for key, value in inputs.items()\n",
    "            }\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "            )\n",
    "            logit = outputs.logits\n",
    "            prob = (\n",
    "                softmax(logit, dim=1).cpu().numpy()\n",
    "            ) \n",
    "            predictions.append(np.argmax(prob))\n",
    "            probs.append(prob)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    return predictions, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656cd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"path_to_model_or_model_id\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"path_to_model_or_model_id\", num_labels = 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935146b0-ce4e-437c-92ae-8d8ac2872ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d88186ed89456faeb2d92f7145c13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52746 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9575bdf0cf4ae4800deb8310c68bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, valid, test = pd.read_csv(\"path_to_train_data\"), pd.read_csv(\"path_to_validation_data\"), pd.read_csv(\"path_to_test_data\")\n",
    "\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": list(train[\"text\"]),\n",
    "        \"label\": list(train[\"locale\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": list(valid[\"text\"]),\n",
    "        \"label\": list(valid[\"locale\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9160ddb0-6c09-482e-b8d9-35414a141f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacollator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "callback = EarlyStoppingCallback(4, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba80982-6aa7-4737-a779-da932b66ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = torch.tensor(p.predictions)\n",
    "    \n",
    "    probs = softmax(preds, dim=1).numpy()\n",
    "    predicted_classes = np.argmax(probs, axis=1)\n",
    "    \n",
    "    return {\n",
    "        \"f1_score\": f1_score(p.label_ids, predicted_classes, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    num_train_epochs=30,\n",
    "    learning_rate=3e-5,\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps=400,\n",
    "    save_steps=400,\n",
    "    output_dir=\"path_to_output\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[callback],\n",
    "    data_collator=datacollator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55fa801b-9385-4c67-bdd1-a7ccfda77a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='4947' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/4947 06:43 < 03:40, 7.93 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.203100</td>\n",
       "      <td>0.134468</td>\n",
       "      <td>0.942235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.113566</td>\n",
       "      <td>0.951723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.120900</td>\n",
       "      <td>0.110407</td>\n",
       "      <td>0.954773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.099610</td>\n",
       "      <td>0.956991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.102611</td>\n",
       "      <td>0.959673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.122075</td>\n",
       "      <td>0.959924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.119699</td>\n",
       "      <td>0.960179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.119592</td>\n",
       "      <td>0.956104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3200, training_loss=0.10467703104019165, metrics={'train_runtime': 404.2132, 'train_samples_per_second': 391.472, 'train_steps_per_second': 12.239, 'total_flos': 3390436834900992.0, 'train_loss': 0.10467703104019165, 'epoch': 1.9405700424499697})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30384ad0-9d27-425c-8a1f-2780ba87cbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17583/17583 [00:58<00:00, 298.65it/s]\n"
     ]
    }
   ],
   "source": [
    "outs, probs = list(predict(list(test[\"text\"]), model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7977ca75-0121-4204-ad9b-be92a954931c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9533011502662643)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(outs, list(test[\"locale\"]), average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d25647-df36-45cf-9112-aae13715ad69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1763/1763 [00:06<00:00, 287.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:AU | Dom:reddit | Split:train | F1:0.9199178753393411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [00:00<00:00, 289.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:AU | Dom:reddit | Split:valid | F1:0.9320682577018967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:01<00:00, 290.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:AU | Dom:reddit | Split:test | F1:0.9346246282944315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [00:03<00:00, 287.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:AU | Dom:google | Split:train | F1:0.9873329652621609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:00<00:00, 288.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:AU | Dom:google | Split:valid | F1:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 287.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:AU | Dom:google | Split:test | F1:0.9723087573554863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1685/1685 [00:05<00:00, 294.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:IN | Dom:reddit | Split:train | F1:0.6898577897862456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:00<00:00, 294.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:IN | Dom:reddit | Split:valid | F1:0.6516573396470081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 479/479 [00:01<00:00, 294.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:IN | Dom:reddit | Split:test | F1:0.6647467882754161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648/1648 [00:05<00:00, 290.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:IN | Dom:google | Split:train | F1:0.9036984689481744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:00<00:00, 290.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:IN | Dom:google | Split:valid | F1:0.9011494252873563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 290.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:IN | Dom:google | Split:test | F1:0.9176345715033853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1007/1007 [00:03<00:00, 292.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:UK | Dom:reddit | Split:train | F1:0.9144587072496614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [00:00<00:00, 291.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:UK | Dom:reddit | Split:valid | F1:0.8820891441071916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [00:00<00:00, 292.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:UK | Dom:reddit | Split:test | F1:0.9120537473648652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1817/1817 [00:06<00:00, 285.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:UK | Dom:google | Split:train | F1:0.9925763194352191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:00<00:00, 285.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:UK | Dom:google | Split:valid | F1:0.9939556858911698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [00:01<00:00, 286.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loc:UK | Dom:google | Split:test | F1:0.9855167267238478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "locs = {\n",
    "    # Inner-circle vs. Outer-circle\n",
    "    \"en-AU\": 0,\n",
    "    \"en-UK\": 0,\n",
    "    \"en-IN\": 1,\n",
    "}\n",
    "for locale in [\"AU\", \"IN\", \"UK\"]:\n",
    "    for dom in [\"reddit\", \"google\"]:\n",
    "        for split in [\"train\", \"valid\", \"test\"]:\n",
    "            temp = pd.read_json(\"path_to_var_data\", lines=True)\n",
    "\n",
    "            temp[\"locale\"] = [locs[\"en-\"+locale]]*len(temp)\n",
    "        \n",
    "            preds, probs= predict(list(temp[\"text\"]), model, tokenizer)  \n",
    "    \n",
    "            probs = [prob[0][locs[\"en-\"+locale]] for prob in probs]\n",
    "            outs = pd.DataFrame({\n",
    "                \"loc\": preds,\n",
    "                \"probs\": probs\n",
    "                }\n",
    "            )\n",
    "            outs.to_csv(\"path_to_results\")\n",
    "\n",
    "            print(f'Loc:{locale} | Dom:{dom} | Split:{split} | F1:{f1_score(list(outs[\"loc\"]), list(temp[\"locale\"]), average=\"weighted\")}')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382cf2da-f11a-42ec-8cfe-bb5bfd29efe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loc</th>\n",
       "      <th>dom</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">en-AU</th>\n",
       "      <th>google</th>\n",
       "      <td>0.990104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit</th>\n",
       "      <td>0.944910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">en-IN</th>\n",
       "      <th>google</th>\n",
       "      <td>0.938977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit</th>\n",
       "      <td>0.775043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">en-UK</th>\n",
       "      <th>google</th>\n",
       "      <td>0.992911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit</th>\n",
       "      <td>0.933627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  prob\n",
       "loc   dom             \n",
       "en-AU google  0.990104\n",
       "      reddit  0.944910\n",
       "en-IN google  0.938977\n",
       "      reddit  0.775043\n",
       "en-UK google  0.992911\n",
       "      reddit  0.933627"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = {\n",
    "    \"loc\":[],\n",
    "    \"dom\": [],\n",
    "    \"split\": [],\n",
    "    \"prob\": []\n",
    "         }\n",
    "\n",
    "for dom in [\"reddit\", \"google\"]:\n",
    "    for loc in [\"au\", \"in\", \"uk\"]:\n",
    "        probs = []\n",
    "        for split in [\"train\", \"test\", \"valid\"]:\n",
    "            pred = pd.read_csv(\"path_to_results\")\n",
    "\n",
    "            metric[\"loc\"] = metric[\"loc\"] + [f\"en-{loc.upper()}\"]*len(pred)\n",
    "            metric[\"dom\"] = metric[\"dom\"] + [dom]*len(pred)\n",
    "            metric[\"split\"] = metric[\"split\"] + [split]*len(pred)\n",
    "            metric[\"prob\"] = metric[\"prob\"] + list(pred.probs)\n",
    "\n",
    "            probs.extend(list(pred.probs))\n",
    "        \n",
    "pd.DataFrame(metric)[[\"loc\", \"dom\", \"prob\"]].groupby([\"loc\", \"dom\"]).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
